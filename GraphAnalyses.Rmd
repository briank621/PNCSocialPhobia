---
title: "GraphAnalyses"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading Libraries

```{r}
library("brainGraph")
library("data.table")
library("MKinfer")
library("pracma")
library("permuco")
```

## Load graphs and variables

List of participants to exclude due to excessive head movement

```{r}
toRemove = read.table("HeadMotionExclude.txt", col.names="SUBJID")
toInclude = read.table("IncludedSubjects.txt", col.names="SUBJID")
FD.dt = read.table("HeadMotion.tsv", sep="\t",fill=TRUE,header=TRUE)
```
Define indices of control and experimental group

```{r}
groups = c('Control', 'SP', 'SPADHD')
#Create a copy of covars.fmri with subjects with excessive head motion removed
inds = lapply(groups, function(x) covars.fmri[status == x, which=TRUE])
#inds = lapply(groups, function(x) covars.fmri[status == x & SUBJID %in% toInclude$SUBJID, which=TRUE])
controlIndexes = inds[[1]]
spPureIndexes = inds[[2]]
spADHDIndexes = inds[[3]]

# Generating all possible control and experimental group pairings
# Control - Experimental
# healthy - spPure + spADHD
# healthy - spPure
# healthy - spADHD
# spADHD - spPure
groupings = c("healthy - spPure + spADHD", "healthy - spPure", "healthy - spADHD", "spADHD - spPure")
allControls = list(controlIndexes, controlIndexes, controlIndexes, spADHDIndexes)
allExperimental = list(c(spPureIndexes, spADHDIndexes), spPureIndexes, spADHDIndexes, spPureIndexes)
```

```{r}
g.all.sub = readRDS(file.path(savedir,'CCAAL.g.all.sub.rds'))
thresholds = seq(.1, .35, .05)
# only looking at subject level graphs for now, instead of group level graphs
# g.all.group = readRDS(file.path(savedir,'g.all.group.rds'))
dt.g.all.sub.graph = rbindlist(lapply(g.all.sub, graph_attr_dt))
dt.g.all.sub.vertex = rbindlist(lapply(g.all.sub, vertex_attr_dt))
```

Combining different datatables
```{r}
#dt.g.all.sub.graph = rbind(dt.g.all.sub.graph, rbindlist(lapply(readRDS(file.path(savedir,'ADHD.g.all.sub.rds')), graph_attr_dt)))
#small.dt = rbind(small.dt, readRDS(file.path(savedir,'adhd.small.dt.rds')))
#dt.g.all.sub.vertex = rbind(dt.g.all.sub.vertex, rbindlist(lapply(readRDS(file.path(savedir,'ADHD.g.all.sub.rds')), vertex_attr_dt)))
```


## Extract Vertex and Graph Level Metrics into Data Table

```{r}
dt.g.all.sub.vertex = rbindlist(lapply(g.all.sub, vertex_attr_dt))
dt.g.all.sub.graph = rbindlist(lapply(g.all.sub, graph_attr_dt))
#dt.g.all.group.vertex = rbindlist(lapply(g.all.group, vertex_attr_dt))
#dt.g.all.group.graph = rbindlist(lapply(g.all.group, graph_attr_dt))
```
## Computing AUCs

All AUC results will be stored in a single data table 

```{r}
AUCResults = data.table()
AUCResults = cbind(AUCResults, covars.fmri[, .(SUBJID, status, smry_soc, age_at_cnb, Sex)])
AUCResults$rowNumber = 1:nrow(AUCResults)
```

### Global Parameters

Largest component sizes were computed as a fraction of all possible

```{r}
dt.g.all.sub.graph$max.frac.comp = dt.g.all.sub.graph[,.(max.comp)] / 116
```

```{r}
source("ComputeAUC.R")
metrics = list("max.frac.comp", "E.local", "E.global", "Lp", "Cp")
for(metric in metrics){
    result = computeAUC(dt.g.all.sub.graph, covars.fmri, thresholds, metric, "SUBJID")
    AUCResults = merge(AUCResults, result)
}
```

#### Small World Global Parameters

### Small Worldness
Computing the small worldness requires generation of random graphs. 

**This code takes a while to run (~ 1hour). As an alternative, load the saved data file, in the code block after this one.**

```{r}
source("ComputeSmall.R")
small.dt = computeSmall(g.all.sub, thresholds, 20)
saveRDS(small.dt, file=file.path(savedir, 'CCAAL.small.dt.rds'), compress='xz')
```

```{r}
## Loading precomputed results
small.dt = readRDS(file.path(savedir,'CCAAL.small.dt.rds'))
```


Compute AUC Results for Small World Statistics
```{r}
source("ComputeAUC.R")
metrics = list("Lp.norm", "Cp.norm", "sigma")
for(metric in metrics){
    result = computeAUC(small.dt, covars.fmri, thresholds, metric, "SUBJID")
    AUCResults = merge(AUCResults, result)
}
```

### Computing PValues for Global Metrics

```{r}
#metrics = list("max.frac.comp", "E.local", "E.global", "Lp", "Cp")
metrics = list("max.frac.comp", "E.local", "E.global", "Lp", "Cp", "Lp.norm", "Cp.norm", "sigma")
groupings = c("healthy - spPure + spADHD", "healthy - spPure", "healthy - spADHD", "spADHD - spPure")
globalPValues = data.table(p.value=numeric(), metric=character(), group=character())[1:(length(metrics)*length(groupings))]

for(i in 1:length(groupings)){
    controlGroup = allControls[[i]]
    experimentalGroup = allExperimental[[i]]
    for(index in 1:length(metrics)){
        metric = metrics[[index]]
        
        spList = AUCResults[experimentalGroup][[metric]]
        controlList = AUCResults[controlGroup][[metric]]
        ttest = perm.t.test(spList, controlList, R = 10000) # two sample t-test (SP, Control)
        globalPValues[(i - 1) * length(metrics) + index] = list(ttest$perm.p.value, metric, groupings[i])
    }
}
```

FDR Correction for all Global Metrics

```{r}
#adjustedGlobalPValues = p.adjust(globalPValues$p.value, "fdr")
#globalPValues[which(adjustedGlobalPValues < .05)]
significantGlobal = data.table(metric=character(), group=character(), diff=character(), pvalue=numeric())
for(i in 1:length(groupings)){
    selectedPValues = globalPValues[group == groupings[i]]
    adjustedGlobalPValues = p.adjust(selectedPValues$p.value, "fdr")
    significantValues = which(adjustedGlobalPValues < .05)
    if(length(significantValues) == 0){
        next
    }
    for(j in 1:length(significantValues)){
        index = significantValues[j]
        metricName = selectedPValues[index]$metric
        controlGroup = allControls[[i]]
        experimentalGroup = allExperimental[[i]]
        diff = mean(AUCResults[[metricName]][controlGroup]) - mean(AUCResults[[metricName]][experimentalGroup])
        significantGlobal = rbind(significantGlobal, list(metricName, groupings[i], diff, adjustedGlobalPValues[index]))
    }
}
```

### ANCOVA

Try ANCOVA with age as a nuisance variable

```{r}
## Centering covariate to the mean
#metrics = list("max.frac.comp", "E.local", "E.global", "Lp", "Cp")
metrics = list("max.frac.comp", "E.local", "E.global", "Lp", "Cp", "Lp.norm", "Cp.norm", "sigma")
AUCResults$age_centered = scale(AUCResults$age_at_cnb, scale = FALSE)
AUCResults$meanFD = FD.dt$mean.FD_Power
AUCResults$percentHighFD = FD.dt$Percent.of.FD_Power.0.5
ancovaPValues = data.table(age=numeric(), status=numeric(), sex=numeric(), meanFD=numeric(), metric=character())[1:length(metrics)]
for(index in 1:length(metrics)){
    metric = metrics[[index]]
    formula = paste(metric, '~age_at_cnb+status+Sex+meanFD',sep = '')
    result = aovperm(as.formula(formula), data=AUCResults, np = 10000)
    rowToAdd = as.double(result$table[['permutation P(>F)']])
    for(pindex in 1:(length(rowToAdd) - 1)) {
        ancovaPValues[index,pindex] = rowToAdd[pindex]
    }
    ancovaPValues[index,NCOL(ancovaPValues)] = metric
}


```

### Regional Parameters

We try and compute parameters for each node. This code takes a little longer as it has to loop through each node and then for each subject. The data is not added to the `AUCResults` table but returns a separate list of pvalues that corresponds to the regions.

### Nodal Global Efficiency

```{r}
source("ComputeRegionalAUC.R")
regionalResults = list()
for(i in 1:length(groupings)){
    controlGroup = allControls[[i]]
    experimentalGroup = allExperimental[[i]]
    eNodalResult = computeRegionalAUC(dt.g.all.sub.vertex, covars.fmri, thresholds, controlGroup, experimentalGroup, "E.nodal")
    localResult = computeRegionalAUC(dt.g.all.sub.vertex, covars.fmri, thresholds, controlGroup, experimentalGroup, "E.local")
    degreeResult = computeRegionalAUC(dt.g.all.sub.vertex, covars.fmri, thresholds, controlGroup, experimentalGroup, "degree")
    regionalResults[[i]] = list("label"=groupings[i], "eNodal"=eNodalResult, "local"=localResult, "degree"=degreeResult)
}


```

```{r}
saveRDS(regionalResults, file=file.path(savedir, 'CCAAL.regionalResults.rds'), compress='xz')
```

```{r}
regionalResults = readRDS(file.path(savedir,'CCAAL.regionalResults.rds'))
```



We can also check nodal statistics individually. This code is primarily used for testing single regional hypotheses.

```{r}
source("ComputeSingleRegionAUC.R")
singleAUCResults =covars.fmri[, .(SUBJID, status, smry_soc)]
singleAUCResults$rowNumber = 1:nrow(AUCResults)
regionName = "SOG.R"
regions = c(regionName)
for(region in regions){
    result = computeSingleRegionAUC(dt.g.all.sub.vertex, region, covars.fmri, thresholds, "E.nodal");
    singleAUCResults = merge(singleAUCResults, result, by=c("SUBJID"))
}
metricName = paste("E.nodal_", regionName, sep="")
spList = singleAUCResults[status=="SPADHD"][[metricName]]
controlList = singleAUCResults[status=="SP"][[metricName]]
# ttest = perm.t.test(spList, controlList, R = 10000) # two sample t-test (SP, Control)
```

## Verify that the threshold is appropriate

The density threshold that we use should satisfy two criteria (Zhu et al.)
1) Average degree over all nodes is larger than 2 * log(n)
2) Small worldness is larger than 1.1 for all participants
3) Largest component size should include most of the graph

The current graph we are using is density = .35

### Average Degree

Computing the minimum average degree among all subject graphs. Since the density has been enforced, the average degree is the same for all graphs (92.053), which is higher than 2 * log(264)

```{r}
validThresholds = list()
numThresholds = length(thresholds)
for(i in 1:numThresholds){
    for(subjectGraph in g.all.sub[[i]]$graphs){
        subjectDegree = mean(degree(subjectGraph))
        minDegree = min(minDegree, subjectDegree)
    }
    if(minDegree > 2 * log(264)){
        validThresholds = append(validThresholds, thresholds[i])
    }
}

```

### Verification of Small World Statistics

Our density threshold should allow all subjects to have a small-world statistic (sigma) of greater 1.1

```{r}
small.dt[,.(minSigma = min(sigma), meanSigma = mean(sigma), stdSigma = sd(sigma)),by=.(threshold)]
```
This assumption is violated, but the average value is greater than 1.1

### Extracting bad subjects

We can try and remove subjects with sigma < 1. You can always reset covars.fmri to the default by loading covariates in `BrainGraphTest.rmd`.

```{r}
badSubjects = unique(small.dt[sigma < 1.1], by = "SUBJID")$SUBJID
covars.fmri = covars.fmri[! SUBJID %in% badSubjects]
```

## Largest Component Size

```{r}
dt.g.all.sub.graph[,.(minComp = min(max.comp), meanComp = mean(max.comp), stdComp = sd(max.comp)),by=.(threshold)]
```
